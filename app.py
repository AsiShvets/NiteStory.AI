from dotenv import find_dotenv, load_dotenv
from transformers import pipeline
from langchain.prompts import PromptTemplate
#from langchain.schema import Runnable
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatOpenAI 
import openai
from openai import OpenAI
#from openai.error import OpenAIError, RateLimitError, AuthenticationError, APIConnectionError
import requests
import os
from langchain.schema import AIMessage
import streamlit as st

# Load environment variables from .env file
load_dotenv(find_dotenv()) 

# Retrieve API token from environment variables
HUGGINGFACE_API_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN") 

if not HUGGINGFACE_API_TOKEN:
    raise ValueError("HUGGINGFACEHUB_API_TOKEN is not set. Check your .env file.")


#Image To Text Generator Model
# The function takes an image path, processes it, and 
# extracts the generated_text key from the model output. 

def img2text(path):
    try:
    # 
    # Generate text caption from an image using a Hugging Face pipeline.
    # Args:
    #     path (str): Path to the image file.
    # Returns:
    #     str: Generated text caption from the image.
       img_to_text = pipeline(

#here we instantiate the model object as img_to_text using the pipeline constructor from hugging face which takes in task (img_to_text) and model name.
            task="image-to-text", 
            model="Salesforce/blip-image-captioning-base",
            token=HUGGINGFACE_API_TOKEN
        )
# it sends the image path to the model via an api call returns the generated text (key: value) and gets stored in the text variable.
       text = img_to_text(path)[0]['generated_text']
       return text
    except FileNotFoundError:
        return "Error: The specified image file was not found."
    except Exception as e:
        return f"Error processing image: {e}"

# we define the story generator function which takes the scenario as 
# an argument. Notice here the scenario refers to the story generated by the model earlier

def story_generator(scenario):
# we define our custom instructions under the variable template with context as the scenario. 
# This is the custom instruction mentioned earlier in the section.
    """
    Generate a short story for kids based on the provided scenario.

    Args:
        scenario (str): The narrative or context for the story.

    Returns:
        str: A short story generated using the GPT-3.5-turbo model.
    """
    
    template = """
    You are an expert kids story teller;
    You can generate short stories based on a simple narrative
    Your story should be more than 50 words.

    CONTEXT: {scenario}
    STORY:
    """
    # we generate a prompt using the hugging face PromptTemplate class. It takes in the template 
    # (entire text) and the custom context (here scenario)
    #Prompt Template is used to create a prompt based on the template / the context provided. 
    # In this case, it specifies there is extra context -scenario.
    prompt = PromptTemplate(template=template, input_variables = ["scenario"])

    # we create an instance of the chat-gpt-3.5-turbo model using LLMChain wrapper from lang-chain. 
    # The model requires a model name, temperature (randomness in response), prompt (our custom prompt), and verbose (to display logs).

    #LLM-Chain is used to represent a chain of LLM models. In our case, it represents the OpenAI language model with GPT 3.5 Turbo model. 
    # In simple terms, you can chain multiple LLMs together.
    story_llm = ChatOpenAI(model_name= 'gpt-3.5-turbo', temperature = 0.7)
    
    # Now we call the model using the predict method and pass the scenario

    story = story_llm.invoke(prompt.format(scenario=scenario))

    # This returns a story based on the context, stored in the story variable
    # In the end, we return the story to pass it to the last model. 
    return story


#text-to-speech (Hugging Face)
#we define a function text2speech whose job is to take in the msg 
#(the story generated from the previous model) and return the audio file.
def text2speech(msg):
    if isinstance(msg, AIMessage):
        msg_content = msg.content
    else:
        msg_content = str(msg)
    # API_URL, which holds the api end-point to call.
    API_URL = "https://api-inference.huggingface.co/models/espnet/kan-bayashi_ljspeech_vits"
   
    # we provide the authorization and bearer token in the header. This will 
    # be provided as a header (authorization data) when we call the model.
    headers = {"Authorization": f"Bearer {HUGGINGFACE_API_TOKEN}"}
    
    # we define a payload dictionary (JSON format) that contains the message (msg) we need to convert
    payloads = {
         "inputs" : msg_content
    }
   
    #posts request to model is sent along with header and JSON data. The returned response is stored in the response variable.
    response = requests.post(API_URL, headers=headers, json=payloads)
   
    # we save the audio filesâ€™ content (response.content) in the local system by writing the required response audio.flac. 
    # This is done for content safety and optional.
    if response.status_code == 200:
        with open('audio.flac', 'wb') as f:
            f.write(response.content)
        return "Audio saved as audio.flac"
    else:
        return f"Error: {response.json().get('error', 'Unknown error')}"


scenario = img2text("img.jpeg") #text2image

#try:
story = story_generator(scenario)  # Generate story
#except RateLimitError:
   # print("Quota exceeded. Please check your OpenAI account.")
#except OpenAIError as e:
   # print(f"An OpenAI error occurred: {e}")

text2speech(story) # convert generated text to audio

def main():
    st.set_page_config(page_title = "AI story Teller", page_icon ="ðŸ¤–")

    st.header("We turn images to story!")
    upload_file = st.file_uploader("Choose an image...", type=['jpg', 'jpeg', 'png'])  #uploads image

    if upload_file is not None:
        print(upload_file)
        binary_data = upload_file.getvalue()
        
        # save image
        with open (upload_file.name, 'wb') as f:
            f.write(binary_data)
        st.image(upload_file, caption = "Image Uploaded", use_container_width = True) # display image

        scenario = img2text(upload_file.name) #text2image
        story = story_generator(scenario) # create a story
        text2speech(story) # convert generated text to audio

        # display scenario and story
        with st.expander("scenario"):
            st.write(scenario)
        with st.expander("story"):
            st.write(story)
        
        # display the audio - people can listen
        st.audio("audio.flac")

# the main
if __name__ == "__main__":
    main()